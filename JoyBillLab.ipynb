{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "\n",
    "data = pd.read_csv(\"/Users/joytruong/Documents/Bill/Datasets/mnist_test.csv\")\n",
    "data = np.array(data)\n",
    "np.random.shuffle(data)\n",
    "    \n",
    "train_n = 1000\n",
    "test_set = data[train_n:].T\n",
    "y_test = test_set[0]\n",
    "x_test = test_set[1:] / 255.0\n",
    "\n",
    "train_set = data[:train_n].T\n",
    "y_train = train_set[0]\n",
    "x_train = train_set[1:] / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        #Status of each step, Add Layers must be fixed before Passing in Data\n",
    "        self.status = [False, False]\n",
    "        #A list of Hidden neurons, ith index is # of Neurons of ith layer\n",
    "        self.hiddenNeurons = []\n",
    "        #A list of Hidden Weights and Biases, ith index is Weight matrix to the next layer\n",
    "        self.weights = []\n",
    "        self.biases  = []\n",
    "        #A list of dZ[i]\n",
    "        self.dZ = []\n",
    "        #A list of A[i], where A[0] is X(the inputs), A[n] is the outputs\n",
    "        self.Z = []\n",
    "        self.A = [] \n",
    "        self.dW = None#New on version 2.0\n",
    "        #A list of Activation Functions\n",
    "        self.activation_functions = []\n",
    "\n",
    "    def set(self, unit:bool, alpha:float, epoch:int, batch_size:int):\n",
    "        #Mandatory\n",
    "        self.unit = unit\n",
    "        self.alpha = alpha\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.status[1] = True\n",
    "\n",
    "    def ReLU(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def ReLU_deriv(self, Z):\n",
    "        return Z > 0\n",
    "    \n",
    "    def SoftMax(self, Z):\n",
    "        A = np.exp(Z) / sum(np.exp(Z))\n",
    "        return A\n",
    "    \n",
    "    def one_hot(self, Y):\n",
    "        one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "        one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "        return one_hot_Y.T\n",
    "    \n",
    "    def get_predictions(self, A):\n",
    "        return np.argmax(A, 0)\n",
    "    \n",
    "    def get_accuracy(self, predictions, actual):\n",
    "        passed = np.sum(predictions == actual)\n",
    "        accuracy = passed / actual.size\n",
    "        return accuracy\n",
    "    \n",
    "    def prepare_batch(self):\n",
    "        self.batches = []\n",
    "        main_size = int(self.samples / self.batch_size)\n",
    "        remainder = self.samples - main_size * self.batch_size\n",
    "        start = 0\n",
    "        for batch_index in range(self.batch_size):\n",
    "            self.batches.append(self.A[0][:,start:start+main_size,None])\n",
    "            start += main_size\n",
    "        if remainder > 0:\n",
    "            self.batches.append(self.A[0][:,start:start+remainder,None])\n",
    "\n",
    "\n",
    "    def forward_propagation(self, i=0):\n",
    "        if i == len(self.weights)-1:\n",
    "            #Last Layers\n",
    "            self.Z[i] = self.weights[i] @ self.A[i]\n",
    "            self.A[i+1] = self.SoftMax(self.Z[i] + self.biases[i])\n",
    "            return\n",
    "        \n",
    "        self.Z[i] = self.weights[i] @ self.A[i]\n",
    "        self.A[i+1] = self.ReLU(self.Z[i] + self.biases[i])\n",
    "        self.forward_propagation(i+1)\n",
    "\n",
    "    def backward_propagation(self, i):\n",
    "        dW = self.dZ[i] @ self.A[i].T\n",
    "        if self.unit:\n",
    "            dW = dW / (dW**2).sum()**0.5\n",
    "        self.weights[i] -= self.alpha * dW\n",
    "        db = 1/self.samples * np.sum(self.dZ[i])\n",
    "        self.biases[i]  -= self.alpha * db\n",
    "        if i == 0:\n",
    "            return\n",
    "        self.dZ[i-1] = self.weights[i].T @ self.dZ[i] * self.ReLU_deriv(self.Z[i-1])\n",
    "        self.backward_propagation(i-1) \n",
    "\n",
    "    def gradient_descent(self):\n",
    "        self.one_hot_Y = self.one_hot(self.y_train)\n",
    "        currentPoch = trange(self.epoch, desc='', leave=True)\n",
    "        for i in currentPoch:\n",
    "            self.dZ[-1] = 1/self.samples * (self.A[-1] - self.one_hot_Y)\n",
    "            self.forward_propagation()\n",
    "            self.backward_propagation(len(self.dZ)-1)\n",
    "            if i % 20 == 0:\n",
    "                predictions = self.get_predictions(self.A[-1])\n",
    "                accuracy = self.get_accuracy(predictions, self.y_train)\n",
    "                currentPoch.set_description(f'Train_Accuracy={accuracy*100:.2f}%')\n",
    "                currentPoch.refresh()\n",
    "\n",
    "    #Generate Neural network Structure\n",
    "    def addLayer(self, neurons, activation_function=None):\n",
    "        if self.status[0] == True: \n",
    "            print('Cannot add more layer')\n",
    "            return\n",
    "        allowed = ['relu','tanh','sigmoid']\n",
    "        self.hiddenNeurons.append(neurons)\n",
    "        if activation_function == None:\n",
    "            self.status[0] = True\n",
    "            return\n",
    "        if activation_function not in allowed:\n",
    "            print('error_activation function not found')\n",
    "        else:\n",
    "            self.activation_functions.append(activation_function)\n",
    "\n",
    "    def init_params(self):\n",
    "        self.weights.append(np.random.rand(self.hiddenNeurons[0] , self.n_in) - 0.5)\n",
    "        self.biases.append(np.random.rand(self.hiddenNeurons[0], 1) - 0.5)\n",
    "        for n in range(len(self.hiddenNeurons)-1):\n",
    "            self.weights.append(np.random.rand(self.hiddenNeurons[n+1],self.hiddenNeurons[n]) - 0.5)\n",
    "            self.biases.append(np.random.rand(self.hiddenNeurons[n+1], 1) - 0.5)\n",
    "            self.Z.append(0)\n",
    "            self.dZ.append(0)\n",
    "            self.A.append(0)\n",
    "        self.Z.append(0)#offset\n",
    "        self.dZ.append(0)#offset\n",
    "        self.A.append(0)#offset\n",
    "        self.status = True\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        if False in self.status:\n",
    "            print('Not ready for training')\n",
    "            return\n",
    "        self.A.append(x_train)\n",
    "        self.y_train = y_train\n",
    "        m, n = x_train.shape\n",
    "        self.n_in  = m\n",
    "        self.samples = n\n",
    "        self.n_out = self.hiddenNeurons[-1]\n",
    "        self.init_params()\n",
    "        self.gradient_descent()\n",
    "\n",
    "    def evaluate(self, x_test, y_test):\n",
    "        self.A[0] = x_test\n",
    "        self.y_train = y_test\n",
    "        self.forward_propagation()\n",
    "        predictions = self.get_predictions(self.A[-1])\n",
    "        accuracy = self.get_accuracy(predictions, y_test)\n",
    "        print(f'Test_Accuracy {(accuracy * 100):.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()\n",
    "nn.set(epoch=1000, alpha=0.01, unit=1, batch=10)\n",
    "nn.addLayer(128, 'relu')\n",
    "nn.addLayer(10)\n",
    "nn.fit(x_train, y_train)\n",
    "# nn.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
